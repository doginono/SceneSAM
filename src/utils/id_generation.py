from src.utils.backproject import *
import numpy as np
import cv2
import torch
import math
from src.utils import backproject
from scipy import signal
import torch.nn.functional as F 
from src.utils.vis import visualizerForIds
import matplotlib.pyplot as plt
import fpsample
from sklearn.cluster import KMeans
import numpy as np

def readDepth(filepath):
    depth = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)
    depth_data = depth.astype(np.float32) / 6553.5
    depth_data = torch.from_numpy(depth_data)
    return depth_data


def createMapping(
    ids1,
    backprojectedSamples,
    zg,
    Depthg,
):
    backprojectedSamples = backprojectedSamples.astype(int)
    # efficient
    # filter out samples outside of image bounds
    condition = (
        (backprojectedSamples[1, :] < 0)
        | (backprojectedSamples[0, :] < 0)
        | (backprojectedSamples[1, :] > 679)
        | (backprojectedSamples[0, :] > 1199)
    )
    filteredIndices = np.where(condition)
    filteredBackProj = backprojectedSamples[:, ~condition]
    #if outside filter
    if len(filteredBackProj[0]) == 0:
        return -1

    depthg = np.array(Depthg[filteredBackProj[1, :], filteredBackProj[0, :]])
    zg = np.delete(zg, filteredIndices)
    depthCheck = depthg - zg

    # depth check indices
    # if depth close enough
    indices = np.where(abs(depthCheck) < 0.1)
    filteredBackProj = filteredBackProj[:, indices]

    if filteredBackProj.size == 0:
        return -1
    ids1_elements = ids1[filteredBackProj[1, :], filteredBackProj[0, :]]
    array, counts = np.unique(ids1_elements, return_counts=True)

    idMostOccuring = int(array[np.argmax(counts)])
    #print(idMostOccuring)
    return idMostOccuring


def readImage(filepath):
    image = cv2.imread(filepath)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    return image


# check one class mapping
# behind or not checkdef create_complete_mapping_of_current_frame(
def checkIfInsideImage(
    backprojectedSamples,
    zg,
    Depthg,
):
    backprojectedSamples = backprojectedSamples.astype(int)
    # efficient
    # filter out samples outside of image bounds
    condition = (
        (backprojectedSamples[1, :] < 0)
        | (backprojectedSamples[0, :] < 0)
        | (backprojectedSamples[1, :] > 679)
        | (backprojectedSamples[0, :] > 1199)
    )
    filteredIndices = np.where(condition)
    filteredBackProj = backprojectedSamples[:, ~condition]
    
    depthg = np.array(Depthg[filteredBackProj[1, :], filteredBackProj[0, :]])
    zg = np.delete(zg, filteredIndices)
    depthCheck = depthg - zg

    indices = np.where(abs(depthCheck) < 0.1)
    filteredBackProj = filteredBackProj[:, indices]
    return filteredBackProj



def update_current_frame(curr_mask, id2id):
    """update curr_mask according to sampleFromCurrentMask

    Args:
        curr_mask (np.array): (W,H) with ids
        id2id (np.array): 1D array where at id2id[i] = value, means that the id i of the current frame has actual id value

    Return:
        np.array (W,H): updated mask
    """
    updated = curr_mask.copy()
    ids = np.unique(curr_mask)
    for id in ids:
        updated[curr_mask == id] = id2id[id]
    return updated


def create_complete_mapping_of_current_frame(
    ids_curr,
    curr_frame_number,
    frame_numbers,
    T,
    K,
    depths,
    segmentations,
    id_counter,
    points_per_instance=5,
):
    """creates a mapping of the current ids to the actual ids according to the past frames;
    This is implemented in a way, that some of the function arguments can be replaced by the frame_reader class from nice_slam,
    in this frame_reader we can also distiguish whether the id will be generated by SAM on the fly or if we have generated them in advance

    Args:
        ids_curr (_type_): _description_
        curr_frame_number (int): current frame number
        frame_numbers (np.array): frame numbers, which will be used for the backprojection
        T (list): list of extrinsic camera poses
        K (np.array): intrinsic camera matrix
        depths (list): contains all depthspath of the frames
        frames (list): contains all rgb image paths
        segmentations (list): contains all segmentations of seen frames
        id_counter (int): the current smallest free id.
        points_per_instance (int, optional): number of points per instance. Defaults to 5.

    Return:
        np.array (2, #num ids in curr_mask): mapping of the current ids to the actual ids according to the past frames
    """
    map = []
    Tf = T[curr_frame_number]
    depthf = readDepth(depths[curr_frame_number])
    unique_ids = np.unique(ids_curr).astype(int)
    max_id = np.max(unique_ids).astype(int)
    map = None
    for num in frame_numbers:
        Tg = T[num]
        # starts counting from 0 so the lenght is 1 more than the actual number of frames
        map_of_frame = np.ones(max_id + 1) * (-1)
        ids_past = segmentations[int(num / points_per_instance)]
        # this gets overwrittâ€šen
        depthg = readDepth(depths[num])
        samplesFromCurrentMask = backproject.sample_from_instances(
            ids_curr, np.max(unique_ids) + 1, points_per_instance
        )
        for instance in unique_ids:
            current = samplesFromCurrentMask[:, :, instance]

            backprojectedSamples, zg = backproject.backproject(
                current, Tf, Tg, K, depthf
            )
            # this overwrites the depthg therefore I calculate it in actual_id # depthg
            actual_id = createMapping(
                ids_past,
                backprojectedSamples,
                zg,
                depthg,
            )
            if actual_id == -1:
                actual_id = id_counter
                id_counter += 1
            map_of_frame[
                instance
            ] = actual_id  # at index i of map_of_frame is the actual id of the instance i in the current frame
        if map is None:
            map = map_of_frame[None]
        else:
            np.concatenate([map, map_of_frame[None]], axis=0)
    map = combineMaps(
        map.T
    )  # at index i of map.T are the actual ids of the instance i according to the past frames
    return map, id_counter


def createReverseMapping(
    ids_curr,
    curr_frame_number,
    earlier_frame_numbers,
    T,
    K,
    depths,
    segmentations,
    id_counter,
    predictor,
    points_per_instance=5,
    current_frame=None,
):
    """creates a mapping of the current ids to the actual ids according to the past frames;
    This is implemented in a way, that some of the function arguments can be replaced by the frame_reader class from nice_slam,
    in this frame_reader we can also distiguish whether the id will be generated by SAM on the fly or if we have generated them in advance

    Args:
        ids_curr (_type_): _description_
        curr_frame_number (int): current frame number
        frame_numbers (np.array): frame numbers, which will be used for the backprojection
        T (list): list of extrinsic camera poses
        K (np.array): intrinsic camera matrix
        depths (list): contains all depthspath of the frames
        frames (list): contains all rgb image paths
        segmentations (list): contains all segmentations of seen frames
        id_counter (int): the current smallest free id.
        points_per_instance (int, optional): number of points per instance. Defaults to 5.

    Return:
        np.array (2, #num ids in curr_mask): mapping of the current ids to the actual ids according to the past frames
    """
    map = []
    T_current = T[curr_frame_number]
    depthf = readDepth(depths[curr_frame_number])
    unique_ids = np.unique(segmentations[0]).astype(int)
    max_id = np.max(unique_ids).astype(int)
    masks = np.full((current_frame.shape[0], current_frame.shape[1]), -1)
    #for num in earlier_frame_numbers:
    for num in earlier_frame_numbers:
        T_earlier = T[num]
        # starts counting from 0 so the lenght is 1 more than the actual number of frames
        ids_past = segmentations[-1]

        # Ground truth depth
        depthEarlier = readDepth(depths[num])
        samplesFromEarlier = sample_from_instances(
            ids_past, np.max(unique_ids) + 1, points_per_instance
        )
        predictor.set_image(current_frame)
        #print(unique_ids)
        for instance in unique_ids:
            earlierFrame = samplesFromEarlier[:, :, instance]
                
            frontProjectedSamples, projDepth = backproject.frontProject(
                earlierFrame, T_current,T_earlier, K, depthEarlier
            )
            #print(frontProjectedSamples)

            mask, _, _ = predictor.predict(
                point_coords=np.array(list(zip(frontProjectedSamples[0].tolist(), frontProjectedSamples[1].tolist()))),
                point_labels=len(frontProjectedSamples[0]) * [1],
                #mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask
                multimask_output=False,
            )
            #print(mask.shape)
            masks[mask.squeeze()] = instance
            # this overwrites the depthg therefore I calculate it in actual_id # depthg
    counter=1
    '''
    while np.isin(-1, masks):
        indices= np.where(masks==-1)
        indices=np.dstack(indices)
        print(indices)
        random_index=indices[np.random.choice(len(indices))]
        print(random_index)
        mask, _, _ = predictor.predict(
            point_coords=np.array(random_index),
            point_labels=len(frontProjectedSamples[0]) * [1],
            multimask_output=False,
        )
        #print(mask.shape)
        #masks[mask.squeeze()] = instance
        masks[random_index] = max_id + counter
        counter+=1
        #print("Negative")
    '''
        
        
    return masks

#sample from earlier frames and predict 
'''
    coarse to fine
    backproject majority vote
    when with automaticsam generator ratio of in points!!!
    reset the samples 
" After creating the mask backproject all ids into it and give according to the projected majority vote "
check if inside mask
take majority vote
'''
def createReverseMappingCombined(
    curr_frame_number,
    T,
    K,
    depths,
    predictor,
    points_per_instance=5,
    current_frame=None,
    samples=None,
    smallesMaskSize=1000,
    kernel_size=80,
    num_of_clusters=4,
):

    T_current = T[curr_frame_number]
    depthf = readDepth(depths[curr_frame_number])
    masks = np.full((current_frame.shape[0], current_frame.shape[1]), -100)
    predictor.set_image(current_frame)
    
    #projected to current camera frame, they also have ids
    frontProjectedSamples, projDepth = backproject.camProject(
        samples, T_current, K
    )
    
    frontProjectedSamples = checkIfInsideImage(frontProjectedSamples, projDepth, depthf)
    #check if all points lie on the same but different mask
    unique_ids = np.unique(frontProjectedSamples[2:,:].astype(int))
    
    #print("unique_ids", unique_ids)
    #temp=[]
    for instance in unique_ids[::-1]:
        
        #sample same id points from from frontProjectedSamples
        filtre = (frontProjectedSamples[2,:,:] == instance)
        #print(count,frontProjectedSamples.shape)
        instanceId = frontProjectedSamples[:, filtre]
        if instanceId.size is not 0 and instanceId[2,0]>=0:
            #print("instanceId",instance)
            theRelevant=np.array(list(zip(instanceId[0].tolist(), instanceId[1].tolist())))
            if len(theRelevant)>=num_of_clusters:
                kmeans = KMeans(n_clusters=num_of_clusters, random_state=0).fit(theRelevant)

                point_coords = kmeans.transform(theRelevant)
                closest_points_indices = np.argsort(point_coords, axis=0)[:1, :]
                closest_points = theRelevant[closest_points_indices]
                sampledPositive=[1]*len(kmeans.cluster_centers_) 
                mask, _, _ = predictor.predict(
                    point_coords=closest_points[0],
                    point_labels=sampledPositive,
                    multimask_output=False,
                )
                #print(closest_points)
                #temp.append([mask,instance])
                masks[mask.squeeze()] = instance
    '''    temp=sorted(temp, key=lambda x: np.count_nonzero(x[0] == 1), reverse=False)
        for element in temp:
            masks[element[0].squeeze()]=element[1]'''
    
    unique_ids = np.unique(masks).astype(int)
    max_id = np.max(masks).astype(int)
    #For cleaning if all the samples are in another mask
    '''for instance in unique_ids:
        filtre = (frontProjectedSamples[2,:,:] == instance)
        instanceId = frontProjectedSamples[:, filtre]
        condition = masks[instanceId[1,:].astype(int), instanceId[0,:].astype(int)] != instance
        #print(frontProjectedSamples.shape)
        
        if condition.size > 0 and np.mean(condition)== 1:
            indices_to_delete = np.where(samples[3,:] == instance)
            samples = np.delete(samples, indices_to_delete, axis=1)
            print("REMOVED", instance)
    unique_ids = np.unique(masks).astype(int)
    '''
    for instance in unique_ids:
        if smallesMaskSize > np.count_nonzero(masks == instance):
            masks[masks == instance] = -100
            
    #print(max_id)
    visualizerForId = visualizerForIds()
    visualizerForId.visualizer(masks)

    
    #sample from the right side
    # for unknown
    #convolve def 
    nb_channels = 1
    h, w = kernel_size,kernel_size
    weights = torch.ones(h, w)
    weights = weights.view(1, 1, h, w).repeat(1, nb_channels, 1, 1).to("cuda")
    torch_masks=torch.from_numpy(masks).float().to("cuda")
    result = F.conv2d(torch_masks.unsqueeze(0), weights, padding=h//2)
    result = result.cpu().numpy()  # convert to numpy array
    result=result.squeeze()
    indices = np.array(np.where(result == (-100*(h*w)))).T
    
    counter=0
    
    if len(indices)>0:
        counter=1
        
        random_index = indices[np.random.choice(len(indices))]
        point_coords = np.array([random_index[1],random_index[0]])
        point_coords = point_coords.reshape(1, -1)
        # print(point_coords)
        mask, _, _ = predictor.predict(
            point_coords=point_coords,
            point_labels=np.array([1]),
            multimask_output=False,
        )
        condition = mask.squeeze() & (masks == -100)

        masks[condition] = max_id + counter
        #need to sample more in the first time I found the new id
    

    max_id = max_id + counter
    numberOfMasks=len(np.unique(masks))
    samplesFromCurrent = sample_from_instances_with_ids(
        masks, 
        numberOfMasks, 
        points_per_instance=20
    )
    #3d
    realWorldProjectCurr = backproject.realWorldProject(
        samplesFromCurrent[:2,:],T[curr_frame_number], K, depthf
    )
    #add the ids 4d
    realWorldProjectCurr = np.concatenate((realWorldProjectCurr,samplesFromCurrent[2:,:]),axis=0)
    samples=np.concatenate((samples,realWorldProjectCurr),axis=1)
    '''
    unique_ids = np.unique(masks).astype(int)
    allsampled=[]
    for i in unique_ids:
        temp = samples[:3, samples[3, :] == i]

        if samples[:2, samples[3, :] == i].size != 0:
            theRelevant = np.array(list(zip(temp[0].tolist(), temp[1].tolist(), temp[2].tolist())))
            fps_indices = fpsample.fps_sampling(theRelevant, min(200, len(theRelevant)))
            fps_samples = theRelevant[fps_indices]
            allsampled.append(fps_samples)

    allsampled = np.concatenate(allsampled, axis=0)
    # Create a mask that checks if each sample in samples is in fps_samples
    mask = np.isin(samples[:3, :].T, allsampled).all(axis=1)
    # Use the mask to index samples
    # samples = samples[:, mask]
    #fps=fpsample.fps_sampling(theRelevant, min(points_per_instance,len(theRelevant)))
    #samples=cleanSamples(samples, T_current, K, depthf, masks)
    print(allsampled.shape)
    print(allsampled)
    #samples=samples[:,allsampled.astype(int)]
    '''
    #print(samples.shape)
    
    return masks,samples

def createReverseReverseMappingCombined(
    curr_frame_number,
    T,
    K,
    depths,
    predictor,
    current_frame=None,
    samples=None,
    smallesMaskSize=1000,
    num_of_clusters=4,
):

    T_current = T[curr_frame_number]
    depthf = readDepth(depths[curr_frame_number])
    masks = np.full((current_frame.shape[0], current_frame.shape[1]), -100)
    predictor.set_image(current_frame)
    
    #projected to current camera frame, they also have ids
    frontProjectedSamples, projDepth = backproject.camProject(
        samples, T_current, K
    )
    
    frontProjectedSamples = checkIfInsideImage(frontProjectedSamples, projDepth, depthf)
    #check if all points lie on the same but different mask
    unique_ids = np.unique(frontProjectedSamples[2:,:].astype(int))
    
    #print("unique_ids", unique_ids)
    #temp=[]
    for instance in unique_ids[::-1]:
        
        #sample same id points from from frontProjectedSamples
        filtre = (frontProjectedSamples[2,:,:] == instance)
        #print(count,frontProjectedSamples.shape)
        instanceId = frontProjectedSamples[:, filtre]
        if instanceId.size is not 0 and instanceId[2,0]>=0:
            #print("instanceId",instance)
            theRelevant=np.array(list(zip(instanceId[0].tolist(), instanceId[1].tolist())))
            if len(theRelevant)>=num_of_clusters:
                kmeans = KMeans(n_clusters=num_of_clusters, random_state=0).fit(theRelevant)

                point_coords = kmeans.transform(theRelevant)
                closest_points_indices = np.argsort(point_coords, axis=0)[:1, :]
                closest_points = theRelevant[closest_points_indices]
                sampledPositive=[1]*len(kmeans.cluster_centers_) 
                mask, _, _ = predictor.predict(
                    point_coords=closest_points[0],
                    point_labels=sampledPositive,
                    multimask_output=False,
                )
                #temp.append([mask,instance])
                masks[mask.squeeze()] = instance
    '''    temp=sorted(temp, key=lambda x: np.count_nonzero(x[0] == 1), reverse=False)
        for element in temp:
            masks[element[0].squeeze()]=element[1]'''
    
    unique_ids = np.unique(masks).astype(int)
    max_id = np.max(masks).astype(int)
    
    
    for instance in unique_ids:
        if smallesMaskSize > np.count_nonzero(masks == instance):
            masks[masks == instance] = -100
            
    #visualizerForId = visualizerForIds()
    #visualizerForId.visualizer(masks)

    
    #sample from the right side
    # for unknown
    #convolve def 
    
    return masks,samples

def cleanSamples(samples,T_current, K, depthf, masks):
    
    frontProjectedSamples, projDepth = backproject.camProject(
        samples, T_current, K
    )
    proj = checkIfInsideImage(frontProjectedSamples, projDepth, depthf)
    unique_ids = np.unique(proj[2,:].astype(int))
    for instance in unique_ids:
        filtre = (proj[2,:,:] == instance)
        # print(count,frontProjectedSamples.shape)
        instance_data = proj[:, filtre]
        # instance_data = proj[:, proj[-1,:].astype(int) == instance]
        if instance_data.shape[0]==frontProjectedSamples.shape[0]:
            indices_to_delete = np.where(samples[3,:] == instance)
            samples = np.delete(samples, indices_to_delete, axis=1)
            #print("REMOVED", instance)
    #print("samples",np.unique(samples[3,:]))
    
    
    return samples
    
    
    
    
    
    
def create_complete_mapping(
    store_path, every_nth_frame, T, K, directory, points_per_instance=5
):
    pass

# when combining the maps, For example we look into earlier 0 5 10 15 20 25 30th frame
# New object recognized in 20th frame, also in 25th frame, 
# Bu when
def combineMaps(map):
    # reduce map on axis 1 end check if all elements are the same or -1
    if map.shape[1] != 1:
        for i in range(map.shape[0]):
            tmp = map[i][map[i] != -1]
            assert ~(np.all(tmp[0] - tmp == 0)), "mapping is not consistent"
    return map.T[0]

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scripts.gifMaker import make_gif_from_array\n",
    "from src.common import as_intrinsics_matrix\n",
    "from torch.utils.data import Dataset\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "from src.utils.datasets import get_dataset\n",
    "import time\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from src.utils import backproject, create_instance_seg, id_generation, vis\n",
    "import argparse\n",
    "from src.NICE_SLAM import NICE_SLAM\n",
    "from src import config\n",
    "import seaborn as sns\n",
    "from src.Segmenter import Segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "description=\"Arguments for running the NICE-SLAM/iMAP*.\"\n",
    ")\n",
    "parser.add_argument(\"config\", type=str, help=\"Path to config file.\")\n",
    "parser.add_argument(\n",
    "    \"--input_folder\",\n",
    "    type=str,\n",
    "    help=\"input folder, this have higher priority, can overwrite the one in config file\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output\",\n",
    "    type=str,\n",
    "    help=\"output folder, this have higher priority, can overwrite the one in config file\",\n",
    ")\n",
    "nice_parser = parser.add_mutually_exclusive_group(required=False)\n",
    "nice_parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "nice_parser.add_argument(\"--imap\", dest=\"nice\", action=\"store_false\")\n",
    "parser.set_defaults(nice=True)\n",
    "args = parser.parse_args(args=['/home/rozenberszki/project/wsnsl/configs/ScanNet/scene0423_02_panoptic.yaml'])\n",
    "#args = parser.parse_args(args=['/home/rozenberszki/project/wsnsl/configs/Own/room0.yaml'])\n",
    "\n",
    "cfg = config.load_config(  # J:changed it to use our config file including semantics\n",
    "        args.config, \"configs/nice_slam_sem.yaml\" if args.nice else \"configs/imap.yaml\"\n",
    "    )\n",
    "slam = NICE_SLAM(cfg, args)\n",
    "frame_reader = get_dataset(cfg, args, cfg[\"scale\"], slam = slam)\n",
    "frame_reader.__post_init__(slam)\n",
    "zero_pos = frame_reader.poses[0]\n",
    "zero_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(object):\n",
    "\n",
    "    def __init__(self, slam, cfg, args, zero_pos, store_directory):\n",
    "        self.smallestMaskSize = 1000\n",
    "        if \"smallestMaskSize\" in cfg[\"Segmenter\"]:\n",
    "            self.smallestMaskSize = cfg[\"Segmenter\"][\"smallestMaskSize\"]\n",
    "        self.store_directory = store_directory\n",
    "        self.zero_pos = zero_pos\n",
    "        os.makedirs(f\"{store_directory}\", exist_ok=True)\n",
    "\n",
    "        self.is_full_slam = cfg[\"Segmenter\"][\"full_slam\"]\n",
    "        self.store_vis = cfg[\"Segmenter\"][\"store_vis\"]\n",
    "        self.use_stored = cfg[\"Segmenter\"][\"use_stored\"]\n",
    "        self.samplePixelFarther=cfg[\"Segmenter\"][\"samplePixelFarther\"]\n",
    "        self.normalizePointNumber=cfg[\"Segmenter\"][\"normalizePointNumber\"]\n",
    "        self.first_min_area = cfg[\"mapping\"][\"first_min_area\"]\n",
    "        #TODO\n",
    "        # self.\n",
    "        \"\"\"path_to_traj = cfg[\"data\"][\"input_folder\"] + \"/traj.txt\"\n",
    "        self.T_wc = np.loadtxt(path_to_traj).reshape(-1, 4, 4)\n",
    "        self.T_wc[:, 1:3] *= -1\"\"\"\n",
    "\n",
    "        self.every_frame = cfg[\"mapping\"][\"every_frame\"]\n",
    "        # self.slam = slam\n",
    "        # self.estimate_c2w_list = slam.estimate_c2w_list\n",
    "        self.id_counter = slam.id_counter\n",
    "        self.idx_mapper = slam.mapping_idx\n",
    "        self.estimate_c2w_list = slam.gt_c2w_list\n",
    "        s = np.ones((4, 4), int)\n",
    "        if cfg[\"dataset\"] == \"tumrgbd\":\n",
    "            s[[0, 0, 1, 2], [0, 1, 2, 2]] *= -1\n",
    "            print(\"tumrgbd\")\n",
    "        elif cfg[\"dataset\"] == \"replica\" or cfg[\"dataset\"] == \"scannet_panoptic\":\n",
    "            s[[0, 0, 1, 1, 2], [1, 2, 0, 3, 3]] *= -1\n",
    "        elif cfg[\"dataset\"] == \"scannet++\":\n",
    "            s[[0, 0, 1, 1, 2,2], [1, 2, 0, 3, 0,3]] *= -1\n",
    "        elif cfg[\"dataset\"] == \"scannet\":\n",
    "            s[[0, 0, 1, 1, 2,2], [1, 2, 0, 3, 0,3]] *= -1\n",
    "\n",
    "        self.shift = s  # s\"\"\"\n",
    "        self.id_counter = slam.id_counter\n",
    "        self.idx_mapper = slam.mapping_idx\n",
    "        # self.idx_coarse_mapper = slam.idx_coarse_mapper\n",
    "\n",
    "        self.every_frame_seg = cfg[\"Segmenter\"][\"every_frame\"]\n",
    "        self.points_per_instance = cfg[\"mapping\"][\"points_per_instance\"]\n",
    "        self.H, self.W, self.fx, self.fy, self.cx, self.cy = (\n",
    "            cfg[\"cam\"][\"H\"],\n",
    "            cfg[\"cam\"][\"W\"],\n",
    "            cfg[\"cam\"][\"fx\"],\n",
    "            cfg[\"cam\"][\"fy\"],\n",
    "            cfg[\"cam\"][\"cx\"],\n",
    "            cfg[\"cam\"][\"cy\"],\n",
    "        )\n",
    "        self.cfg = cfg\n",
    "        self.update_cam()\n",
    "        self.K = as_intrinsics_matrix([self.fx, self.fy, self.cx, self.cy])\n",
    "        if args is None or args.input_folder is None:\n",
    "            self.input_folder = cfg[\"data\"][\"input_folder\"]\n",
    "        else:\n",
    "            self.input_folder = args.input_folder\n",
    "        # self.color_paths = sorted(glob.glob(f\"{self.input_folder}/results/frame*.jpg\"))\n",
    "        # self.depth_paths = sorted(glob.glob(f\"{self.input_folder}/results/depth*.png\"))\n",
    "        self.frame_reader = get_dataset(\n",
    "            cfg,\n",
    "            args,\n",
    "            cfg[\"scale\"],\n",
    "            device=cfg[\"mapping\"][\"device\"],\n",
    "            tracker=False,\n",
    "            slam=slam,\n",
    "        )\n",
    "        self.n_img = self.frame_reader.n_img\n",
    "        self.semantic_frames = slam.semantic_frames\n",
    "        self.idx_segmenter = slam.idx_segmenter\n",
    "        if not self.is_full_slam:\n",
    "            self.idx = torch.tensor([self.n_img])\n",
    "        else:\n",
    "            self.idx = slam.idx  # Tracking index\n",
    "            # Segmenter index\n",
    "        # self.new_id = 0\n",
    "        self.visualizer = vis.visualizerForIds()\n",
    "        self.frame_numbers = []\n",
    "        self.samples = None\n",
    "        self.deleted = {}\n",
    "    # TODO CHANGE THIS just for now rotated\n",
    "        self.border = (\n",
    "            cfg[\"Segmenter\"][\"border\"]\n",
    "            if cfg[\"Segmenter\"][\"border\"]\n",
    "            else \"crop_edge\" in cfg[\"cam\"]\n",
    "        )\n",
    "        self.num_clusters = cfg[\"Segmenter\"][\"num_clusters\"]\n",
    "        self.overlap = cfg[\"Segmenter\"][\"overlap\"]\n",
    "        self.relevant = cfg[\"Segmenter\"][\"relevant\"]\n",
    "        self.max_id = 0\n",
    "        self.update = {}\n",
    "        self.verbose = cfg[\"Segmenter\"][\"verbose\"]\n",
    "        self.merging_parameter = cfg[\"Segmenter\"][\"merging_parameter\"]\n",
    "        self.hit_percent = cfg[\"Segmenter\"][\"hit_percent\"]\n",
    "        self.depthCondition = cfg[\"Segmenter\"][\"depthCondition\"]\n",
    "\n",
    "    def update_cam(self):\n",
    "        \"\"\"\n",
    "        Update the camera intrinsics according to pre-processing config,\n",
    "        such as resize or edge crop.\n",
    "        \"\"\"\n",
    "        # resize the input images to crop_size (variable name used in lietorch)\n",
    "        if \"crop_size\" in self.cfg[\"cam\"]:\n",
    "            crop_size = self.cfg[\"cam\"][\"crop_size\"]\n",
    "            sx = crop_size[1] / self.W\n",
    "            sy = crop_size[0] / self.H\n",
    "            self.fx = sx * self.fx\n",
    "            self.fy = sy * self.fy\n",
    "            self.cx = sx * self.cx\n",
    "            self.cy = sy * self.cy\n",
    "            self.W = crop_size[1]\n",
    "            self.H = crop_size[0]\n",
    "\n",
    "        # croping will change H, W, cx, cy, so need to change here\n",
    "        if self.cfg[\"cam\"][\"crop_edge\"] > 0:\n",
    "            self.H -= self.cfg[\"cam\"][\"crop_edge\"] * 2\n",
    "            self.W -= self.cfg[\"cam\"][\"crop_edge\"] * 2\n",
    "            self.cx -= self.cfg[\"cam\"][\"crop_edge\"]\n",
    "            self.cy -= self.cfg[\"cam\"][\"crop_edge\"]\n",
    "\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    def segment_idx_forAuto(self, idx):\n",
    "        \"\"\"img = cv2.imread(self.color_paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\"\"\"\n",
    "        img, depth = self.frame_reader.get_colorAndDepth(idx)\n",
    "        img = (img.cpu().numpy() * 255).astype(np.uint8)\n",
    "        '''print(len(self.estimate_c2w_list))\n",
    "        print(self.border)\n",
    "        print(self.estimate_c2w_list[0])\n",
    "        print(self.estimate_c2w_list[1])'''\n",
    "\n",
    "        masksCreated, s, max_id = id_generation.createFrontMappingAutosort(\n",
    "            idx,\n",
    "            self.estimate_c2w_list* self.shift,\n",
    "            self.K,\n",
    "            depth.cpu(),\n",
    "            self.predictor,\n",
    "            max_id=self.max_id,\n",
    "            current_frame=img,\n",
    "            samples=self.samples,\n",
    "            smallesMaskSize=self.smallestMaskSize,\n",
    "            border=self.border,\n",
    "            depthCondition=self.depthCondition,\n",
    "            samplePixelFarther=self.samplePixelFarther,\n",
    "            normalizePointNumber=self.normalizePointNumber,\n",
    "           # verbose=True  \n",
    "        )\n",
    "\n",
    "        self.samples = s\n",
    "        self.max_id = max_id\n",
    "\n",
    "        frame = torch.from_numpy(masksCreated)\n",
    "        adjusted_index = min(idx // self.every_frame_seg, len(self.semantic_frames) - 1)\n",
    "\n",
    "        self.semantic_frames[adjusted_index] = frame\n",
    "        return frame\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def segment_first_ForAuto(self):\n",
    "        \"\"\"color_path = self.color_paths[0]\n",
    "        color_data = cv2.imread(color_path)\n",
    "        image = cv2.cvtColor(color_data, cv2.COLOR_BGR2RGB)\"\"\"\n",
    "        image, depth = self.frame_reader.get_colorAndDepth(0)\n",
    "        image = (image.cpu().numpy() * 255).astype(np.uint8)\n",
    "        sam = create_instance_seg.create_sam_forauto(\"cuda\")\n",
    "        masks = sam.generate(image)\n",
    "        del sam\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        ids = backproject.generateIds_Auto(\n",
    "            masks, depth.cpu(), min_area=self.first_min_area, samplePixelFarther=self.samplePixelFarther,\n",
    "        )\n",
    "        if self.border != 0:\n",
    "            ids[0 : 2 * self.border ] = -100\n",
    "            ids[-2 * self.border  :] = -100\n",
    "            ids[:, 0 : 2 * self.border ] = -100\n",
    "            ids[:, -2 * self.border  :] = -100\n",
    "        # visualizerForId = vis.visualizerForIds()\n",
    "        # visualizerForId.visualize(ids, f'{self.store_directory}/first_segmentation.png')\n",
    "        #ids[depth.cpu() == 0] = -100\n",
    "        self.semantic_frames[0] = torch.from_numpy(ids)\n",
    "        self.frame_numbers.append(0)\n",
    "        self.max_id = ids.max()\n",
    "        visualizerForId = vis.visualizerForIds()  \n",
    "        visualizerForId.visualizer(\n",
    "            self.semantic_frames[0],\n",
    "            path=f\"/home/rozenberszki/D_Project/wsnsl/output/Scannet++/56a0ec536c/segmentations/0seg_{0}.png\",\n",
    "        )\n",
    "        samplesFromCurrent = backproject.sample_from_instances_with_ids_area(\n",
    "            ids=ids,\n",
    "            normalizePointNumber=self.normalizePointNumber, \n",
    "        )\n",
    "        # changed\n",
    "        #self.zero_pos[:3,3]*=0.5\n",
    "        #self.zero_pos*=self.shift\n",
    "        realWorldSamples = backproject.realWorldProject(\n",
    "            samplesFromCurrent[:2, :],\n",
    "            self.estimate_c2w_list[0]* self.shift,\n",
    "            self.K,\n",
    "            depth.cpu(),\n",
    "        )\n",
    "        print(\"samplesFromCurrent: \", np.unique(samplesFromCurrent[2:, :]))\n",
    "        realWorldSamples = np.concatenate(\n",
    "            (realWorldSamples, samplesFromCurrent[2:, :]), axis=0\n",
    "        )\n",
    "        \n",
    "        return realWorldSamples\n",
    "\n",
    "\n",
    "    def process_frames(self, semantic_frames):\n",
    "        \"\"\"process the semantic ids such that we have the minimum max_id number, eg, ids are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,..,max_id\"\"\"\n",
    "        ids = np.unique(semantic_frames)\n",
    "        result = semantic_frames.clone()\n",
    "        for i in range(len(ids)):\n",
    "            result[semantic_frames == ids[i]] = i\n",
    "        result[semantic_frames == -100] = -100\n",
    "        semantic_frames[:, :, :] = result\n",
    "        return len(ids) - 1\n",
    "\n",
    "    \n",
    "    def runAuto(self, max=-1):\n",
    "        \n",
    "        if self.use_stored:\n",
    "            index_frames = np.arange(0, self.n_img, self.every_frame_seg)\n",
    "            for index in tqdm(index_frames, desc=\"Loading stored segmentations\"):\n",
    "                path = os.path.join(self.store_directory, f\"seg_{index}.npy\")\n",
    "                self.semantic_frames[index // self.every_frame_seg] = torch.from_numpy(\n",
    "                    np.load(path).astype(np.int32)\n",
    "                )\n",
    "            if self.n_img - 1 % self.every_frame_seg != 0:\n",
    "                path = os.path.join(self.store_directory, f\"seg_{self.n_img - 1}.npy\")\n",
    "                self.semantic_frames[-1] = torch.from_numpy(\n",
    "                    np.load(path).astype(np.int32)\n",
    "                )\n",
    "            self.idx_segmenter[0] = self.n_img\n",
    "            return self.semantic_frames, self.semantic_frames.max() + 1\n",
    "\n",
    "        visualizerForId = vis.visualizerForIds()\n",
    "        #self.estimate_c2w_list[:,:3,3]*=0\n",
    "        print(\"segment first frame\")\n",
    "        s = self.segment_first_ForAuto()\n",
    "        visualizerForId.visualizer(\n",
    "            self.semantic_frames[0],\n",
    "            path=f\"/home/rozenberszki/D_Project/wsnsl/output/Own/segmentationScannet/0seg_{0}.png\",\n",
    "        )\n",
    "        print(\"finished segmenting first frame\")\n",
    "        if self.store_vis:\n",
    "            visualizerForId = vis.visualizerForIds()\n",
    "            \"\"\" visualizerForId.visualize(\n",
    "                self.semantic_frames[0],\n",
    "            )\n",
    "            plt.show()\"\"\"\n",
    "        if self.is_full_slam:\n",
    "            path = os.path.join(self.store_directory, f\"seg_{0}.npy\")\n",
    "            # np.save(path, self.semantic_frames[0].numpy())\n",
    "            self.idx_segmenter[0] = 0\n",
    "        self.samples = s\n",
    "        self.predictor = create_instance_seg.create_sam_forauto(\"cuda\")\n",
    "        # create sam\n",
    "        if max == -1:\n",
    "            index_frames = np.arange(\n",
    "                self.every_frame_seg, self.n_img, self.every_frame_seg\n",
    "            )\n",
    "        else:\n",
    "            index_frames = np.arange(self.every_frame_seg, max, self.every_frame_seg)\n",
    "            index_frames_predict = np.setdiff1d(\n",
    "                np.arange(self.every_frame, max, self.every_frame), index_frames\n",
    "            )\n",
    "        for idx in tqdm(index_frames, desc=\"Segmenting frames\"):\n",
    "            print(\"start segmenting frame: \", idx)\n",
    "            Starttime=time.time()\n",
    "            self.segment_idx_forAuto(idx)\n",
    "            stopTime=time.time()\n",
    "            print(\"time taken for segmenting frame: \", stopTime-Starttime)\n",
    "            print(\"finished segmenting frame: \", idx)\n",
    "            \"\"\"visualizerForId.visualize(\n",
    "                self.semantic_frames[idx // self.every_frame_seg]\n",
    "            )\n",
    "            plt.show()\"\"\"\n",
    "            if self.is_full_slam:\n",
    "                self.idx_segmenter[0] = idx\n",
    "            # self.plot()\n",
    "            # print(f'outside samples: {np.unique(self.samples[-1])}')\n",
    "        if self.n_img - 1 % self.every_frame_seg != 0:\n",
    "            _ = self.segment_idx_forAuto(self.n_img - 1)\n",
    "            self.idx_segmenter[0] = self.n_img - 1\n",
    "            if self.store_vis:\n",
    "                visualizerForId.visualize(\n",
    "                    self.semantic_frames[-1],\n",
    "                    path=f\"{self.store_directory}/seg_{self.n_img - 1}.png\",\n",
    "                )\n",
    "\n",
    "        \n",
    "        del self.predictor\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if not self.is_full_slam:\n",
    "            self.max_id = self.process_frames(self.semantic_frames)\n",
    "\n",
    "        if self.store_vis:\n",
    "            index_frames = np.arange(0, self.n_img, self.every_frame_seg)\n",
    "            if self.n_img - 1 % self.every_frame_seg != 0:\n",
    "                index_frames = np.concatenate((index_frames, [self.n_img - 1]))\n",
    "            make_gif_from_array(\n",
    "                self.semantic_frames[index_frames // self.every_frame_seg],\n",
    "                os.path.join(self.store_directory, \"segmentation.gif\"),\n",
    "            )\n",
    "        # store the segmentations, such that the dataset class (frame_reader) could load them\n",
    "        # maybe the stored segmentations can be used for loading segmentations\n",
    "        if False:\n",
    "            for index in tqdm([0] + list(index_frames), desc=\"Storing segmentations\"):\n",
    "                path = os.path.join(self.store_directory, f\"seg_{index}.npy\")\n",
    "                np.save(\n",
    "                    path, self.semantic_frames[index // self.every_frame_seg].numpy()\n",
    "                )\n",
    "\n",
    "        if False:\n",
    "            for index in tqdm([0] + list(index_frames), desc=\"Storing visualizations\"):\n",
    "                path = os.path.join(self.store_directory, f\"seg_{index}.png\")\n",
    "                self.visualizer.visualize(\n",
    "                    self.semantic_frames[index // self.every_frame_seg].numpy(),\n",
    "                    path=path,\n",
    "                )\n",
    "        # EDIT THIS\n",
    "        '''make_gif_from_array(\n",
    "            self.semantic_frames[index_frames // self.every_frame_seg],\n",
    "            os.path.join(self.store_directory, \"segmentation.gif\"),\n",
    "        )'''\n",
    "        return self.semantic_frames, self.max_id + 1\n",
    "\n",
    "    def plot(self):\n",
    "        data = self.samples.copy()\n",
    "        data = data[:, data[1] > -2]\n",
    "        data = self.samples.copy()\n",
    "        data = data[:, data[1] > -2]\n",
    "        x = data[0]\n",
    "        y = data[1]\n",
    "        z = data[2] * -1\n",
    "        z = data[2] * -1\n",
    "        labels = data[3]\n",
    "\n",
    "        # Create a scatter plot\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "        # Plot each point with a color corresponding to its label\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            indices = np.where(labels == label)\n",
    "            ax.scatter(x[indices], y[indices], z[indices], s=3)\n",
    "\n",
    "        # Set axis labels\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        ax.set_zlabel(\"Z\")\n",
    "        ax.set_ylim((-2, 2))\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        ax.set_zlabel(\"Z\")\n",
    "        ax.set_ylim((-2, 2))\n",
    "        # Add a legend\n",
    "        ax.legend()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_reader.get_colorAndDepth(0)[1].cpu())\n",
    "plt.scatter(600,40)\n",
    "plt.show()\n",
    "plt.imshow(frame_reader.get_colorAndDepth(0)[0].cpu())\n",
    "plt.scatter(600,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter(slam, cfg, args, zero_pos, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter.estimate_c2w_list = np.concatenate([p[None] for p in segmenter.frame_reader.poses], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "folder_path = '/home/rozenberszki/project/wsnsl/test'\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(folder_path):\n",
    "    # Delete the folder and its contents\n",
    "    shutil.rmtree(folder_path)\n",
    "\n",
    "# Create the folder\n",
    "os.makedirs(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter.runAuto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "points = np.random.rand(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "\n",
    "# Step 2: Compute a mesh using the Ball Pivoting algorithm\n",
    "# You may need to adjust the radii parameter based on the scale of your point cloud\n",
    "radii = [0.005, 0.01, 0.02, 0.04]\n",
    "mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(\n",
    "       pcd, o3d.utility.DoubleVector(radii))\n",
    "\n",
    "# Step 3: Simplify the mesh (optional, if you need to reduce the complexity)\n",
    "mesh_simplified = mesh.simplify_quadric_decimation(target_number_of_triangles=1000)\n",
    "o3d.io.write_triangle_mesh(\"test_mesh.obj\", mesh_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_reader.n_img//50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for idx in range(0, frame_reader.n_img, 50):\n",
    "    _,_, depth,_,_ = frame_reader[idx]\n",
    "    sns.heatmap(depth.cpu().numpy() == 0)\n",
    "    plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color, depth= frame_reader.get_colorAndDepth(1000)\n",
    "#_,depth = frame_reader.get_colorAndDepth(80)\n",
    "color = color.cpu().numpy()\n",
    "depth = depth.cpu().numpy()\n",
    "depth = depth / np.max(depth)\n",
    "depth = np.stack([depth, depth, depth], axis=-1)\n",
    "fig, ax = plt.subplots(3, 1, figsize=(40, 13))\n",
    "ax[0].imshow(depth)\n",
    "#plt.show()\n",
    "ax[1].imshow(color)\n",
    "#plt.show()\n",
    "ax[2].imshow(color*depth*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsnsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
